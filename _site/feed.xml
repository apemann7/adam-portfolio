<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adam Pollack</title>
    <description></description>
    <link>https://apollack11.github.io/</link>
    <atom:link href="https://apollack11.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 18 Mar 2017 15:52:22 -0500</pubDate>
    <lastBuildDate>Sat, 18 Mar 2017 15:52:22 -0500</lastBuildDate>
    <generator>Jekyll v3.4.0</generator>
    
      <item>
        <title>Exploring Methods for Object Recognition</title>
        <description>&lt;p&gt;Implementation of the Bag of Words method for object recognition.&lt;/p&gt;

&lt;p&gt;Final GitHub Repo: &lt;a href=&quot;https://github.com/apollack11/object-recognition&quot;&gt;object-recognition&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-goal&quot;&gt;The Goal&lt;/h3&gt;
&lt;p&gt;I hadn’t gotten a chance to explore machine learning yet at Northwestern, so I decided to center my winter project around the topic. Exploring object recognition seemed like a straightforward yet interesting way to do so. I set out to learn the basics of machine learning and to create a fully-functioning object classifier.&lt;/p&gt;

&lt;h3 id=&quot;methodology&quot;&gt;Methodology&lt;/h3&gt;

&lt;h4 id=&quot;bag-of-words&quot;&gt;Bag of Words&lt;/h4&gt;
&lt;p&gt;The gif above this post shows the results of a trained classifier using the bag of words method. Bag of words is a four step process which consists of the following&lt;sup&gt;[1]&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STEP 1:&lt;/strong&gt; Extract Features&lt;br /&gt;
This step aims to simplify images into a set of smaller features so the classifier doesn’t need to train on every pixel of every image. These features should be unique and should not depend on orientation or scaling. For this step I used the scale-invariant feature transform (SIFT) algorithm implemented through OpenCV. SIFT features are invariant to orientation and uniform scaling and are partially invariant to illumination changes and affine distortion&lt;sup&gt;[2]&lt;/sup&gt;. Due to their robustness, SIFT features seemed ideal for a project which will be demonstrated in a variety of locations with varying lighting conditions and cameras.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STEP 2:&lt;/strong&gt; Learn Visual Vocabulary&lt;br /&gt;
The next step is to learn visual vocabulary from the features extracted in Step 1. To do this, the features are put into clusters using unsupervised learning. Specifically, this step with use K-Means Clustering to determine features which are related to each other. I used K-Means Clustering from the Scikit Learn Python package to do this.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STEP 3:&lt;/strong&gt; Quantize Features using Visual Vocabulary&lt;br /&gt;
Created a histogram of all the features&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STEP 4:&lt;/strong&gt; Train Classifier&lt;br /&gt;
Implemented a Support Vector Machine (SVM) through Scikit Learn which was trained on the extracted features.&lt;/p&gt;

&lt;h4 id=&quot;results&quot;&gt;Results&lt;/h4&gt;
&lt;p&gt;The results of training on Dataset #2 are shown in the gif above. The details surrounding each dataset are below.&lt;/p&gt;

&lt;p&gt;Dataset #1
    - Contains images taken from phone of soda can and screwdriver
    - Training set
        - 51 images of soda can, 35 of screwdriver
    - Test set
        - 5 images of each
    - Accuracy: 0.859375 —&amp;gt; 85.94%&lt;/p&gt;

&lt;p&gt;Dataset #2
    - Contains images taken from webcam of soda can and screwdriver
    - Training set
        - 193 images of each (85.78% of data)
    - Test set
        - 32 images of each (14.22% of data)
    - Accuracy: 0.953125 —&amp;gt; 95.31%&lt;/p&gt;

&lt;h4 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h4&gt;
&lt;p&gt;Relevant GitHub Repo:
&lt;a href=&quot;https://github.com/apollack11/learning-tensorflow&quot;&gt;learning-tensorflow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I also started looking into object recognition through Tensorflow. I started by completing the tutorials on Tensorflow found &lt;a href=&quot;https://www.tensorflow.org/tutorials/deep_cnn&quot;&gt;here&lt;/a&gt;. One of the tutorials focused on recognizing handwritten digits using data from the MNIST dataset. This problem used a convolutional neural network to train a classifier to recognize handwritten digits. I extended the original solution to save the variables that defined the convolutional neural network so the model could be trained and then used to predict based on individual images. I was able to draw handwritten digits in GIMP and predict the number in the image based on the network. &lt;!-- Below is an example image with prediction. --&gt;&lt;/p&gt;

&lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Write custom SVM to train object recognition&lt;/li&gt;
  &lt;li&gt;Improve robustness by collecting a wider variety of images&lt;/li&gt;
  &lt;li&gt;Implement object recognition with the Baxter robot in our lab&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;br /&gt;
[1] Kitani, Kris. “Bag-of-Visual-Words.” &lt;em&gt;Carnegie Mellon University&lt;/em&gt;. Web. &lt;a href=&quot;http://www.cs.cmu.edu/~16385/lectures/Lecture12.pdf&quot;&gt;http://www.cs.cmu.edu/~16385/lectures/Lecture12.pdf&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Lowe, David G. “Object Recognition from Local Scale-Invariant Features.” &lt;em&gt;Proceedings of the Seventh IEEE International Conference on Computer Vision&lt;/em&gt; (1999): n. pag. Web.&lt;/p&gt;
</description>
        <pubDate>Fri, 17 Mar 2017 22:30:00 -0500</pubDate>
        <link>https://apollack11.github.io/object-recognition.html</link>
        <guid isPermaLink="true">https://apollack11.github.io/object-recognition.html</guid>
        
        <category>Machine</category>
        
        <category>Learning</category>
        
        
        <category>Python</category>
        
        <category>ScikitLearn</category>
        
        <category>Tensorflow</category>
        
      </item>
    
      <item>
        <title>Hockey Simulation</title>
        <description>&lt;p&gt;Mathematica simulation of a hockey puck hitting the crossbar of a goal.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/hockey_simulation.gif&quot; alt=&quot;Hockey Simulation&quot; style=&quot;width: auto; padding: 10px&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;description&quot;&gt;Description&lt;/h3&gt;
&lt;p&gt;The goal of this project was to simulate a hockey stick striking a puck at an angle and having the puck hit the crossbar of a goal. The simulation models the system from the side. From this view, there is a hockey stick, a puck, and a goal. The hockey stick is modeled as a pendulum which has an external force acting on it to simulate a player swinging a real hockey stick. The puck is a rectangle from the side view and thus has rotational and translational kinetic energy upon being hit and hitting the crossbar. In reality, the flex of a hockey stick upon impact with the puck would cause the puck to gain the lift necessary for it to travel from ice-level to the height of the crossbar. To simplify things in this simulation, the puck has an initial angle necessary to cover that vertical distance. The crossbar itself is a cylindrical object which viewed from the side is a circle.&lt;/p&gt;

&lt;h3 id=&quot;euler-lagrange-equations&quot;&gt;Euler-Lagrange Equations&lt;/h3&gt;
&lt;p&gt;This dynamic system was modeled using the Euler-Lagrange method which was calculated using the kinetic energy and potential energy of both the stick and the puck. For both I used the twist matrix (V) and the inertia matrix (Itot) to calculate kinetic energy and I used the mass of each and their vertical positions (y) to calculate potential energy.&lt;/p&gt;

&lt;h3 id=&quot;constraints&quot;&gt;Constraints&lt;/h3&gt;
&lt;p&gt;There are two impacts in the system: one in which the hockey stick strikes the puck and another in which the hockey puck strikes the crossbar. For the first impact, I attempted to model my constraint as point at which the front edge of the hockey stick meets the rear edge of the puck. For the second impact, I used the point at which the front edge of the puck meets the x-location of the front of the crossbar as my constraint.&lt;/p&gt;

&lt;h3 id=&quot;external-forces&quot;&gt;External Forces&lt;/h3&gt;
&lt;p&gt;The hockey stick has a constant force applied of 243.5N. This force was
chosen to create a tangential velocity of 100mph (44.704m/s) at the end of the stick when it
contacts the puck.&lt;/p&gt;
</description>
        <pubDate>Wed, 21 Dec 2016 04:46:00 -0600</pubDate>
        <link>https://apollack11.github.io/hockey-simulation.html</link>
        <guid isPermaLink="true">https://apollack11.github.io/hockey-simulation.html</guid>
        
        <category>dynamics</category>
        
        
        <category>mathematica</category>
        
        <category>Euler-Lagrange</category>
        
      </item>
    
      <item>
        <title>Baxter Pick and Place</title>
        <description>&lt;p&gt;Worked with a team to program a Baxter robot to play the Shell Game using the Robot Operating System.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/BlakeStrebel/shell_game&quot;&gt;Github Repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;More info coming soon!&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Dec 2016 07:26:34 -0600</pubDate>
        <link>https://apollack11.github.io/baxter-pick-and-place.html</link>
        <guid isPermaLink="true">https://apollack11.github.io/baxter-pick-and-place.html</guid>
        
        <category>robotics</category>
        
        
        <category>ROS</category>
        
        <category>python</category>
        
        <category>baxter</category>
        
        <category>git</category>
        
      </item>
    
      <item>
        <title>Shape Stream</title>
        <description>&lt;p&gt;Shape Stream is a cross-platform mobile game for Android and iOS created in Java. The game was released in April 2016 and has over 400 total downloads.&lt;/p&gt;

&lt;p&gt;I began working on Shape Stream in January 2016 with Tyler Carrara and Eric Robinson, two friends from high school. We wanted to learn how to bring an application from idea to completion and thought that creating a mobile game would be an effective way to do so. None of us majored in Computer Science, but we were all familiar with programming and willing to learn the object-oriented programming necessary to produce the game. The project began with an idea of having the user interact with shapes falling from the top of the screen. Building on this idea, we brainstormed exactly what we wanted to include in the game. We determined how the user would score points, how they would lose lives, when the game would end, what power-ups we should include, and more.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;p style=&quot;font-weight: bold&quot;&gt;Shape Stream Initial Concept Images&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/concept-image1.png&quot; alt=&quot;Shape Stream Concept 1&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/concept-image2.png&quot; alt=&quot;Shape Stream Concept 2&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/concept-image3.png&quot; alt=&quot;Shape Stream Concept 3&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;After sketching out a broad concept and listing features, we began laying out a plan for programming the application. We conducted research to figure out what resources we wanted to use to create the game. Because I already had an intermediate knowledge of Java, we chose to use LibGDX as our game engine and RoboVM to port the game from Android to iOS. These technologies allowed us to focus development all in Java and still create a product which could be released in both app marketplaces. Tyler and I were the ones focused on programming, with Tyler learning Java from scratch and me taking my intermediate knowledge of Java and learning how to apply it to the LibGDX game engine and RoboVM.&lt;/p&gt;

&lt;p&gt;Once we had a basic idea of how the programming was going to work, we created a project and a git repository to store our code and collaborate. Using GitHub, we were able to simultaneously make changes to the project and then merge them together. The basic structure of our project consists of a core directory which contains all code related directly to the game. The core directory includes all menus, text overlays, settings, and the game itself. The project also has desktop, android, and ios directories which allow the game to be instantiated on different devices. The desktop version was used for testing new features related directly to the game, and we used Android and iOS devices to test ads, in-app purchases, and the functionality of the game on touch platforms.&lt;/p&gt;

&lt;p&gt;After three months of work including working through spring break, we launched Shape Stream for Android and iOS on April 16th, 2016. On April 15th, 2016, we participated in the MobiLEHIGH game creation competition at Lehigh University and won first prize by choice of the judges.&lt;/p&gt;

&lt;p&gt;Shape Stream involved combining a wide set of skills in order to move from concept to product. In addition to the code, we also created every asset from scratch including music, sound effects, logos, and sprites. I learned an immense amount about game design, project planning, Java programming, bug fixing, and testing while working on this project.&lt;/p&gt;

&lt;p&gt;We received great feedback from a number of friends and are currently working on a new version to improve upon the original concept. If you want to checkout Shape Stream, links to both app stores are at the top of the post.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;p style=&quot;font-weight: bold&quot;&gt;Shape Stream Final Images&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/ShapeStreamScreenshot1.png&quot; alt=&quot;Shape Stream Screenshot 1&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/ShapeStreamScreenshot3.png&quot; alt=&quot;Shape Stream Screenshot 2&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/ShapeStreamScreenshot4.png&quot; alt=&quot;Shape Stream Screenshot 3&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/ShapeStreamScreenshot5.png&quot; alt=&quot;Shape Stream Screenshot 4&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;iOS: &lt;a href=&quot;https://itunes.apple.com/td/app/shape-stream/id1095099500?mt=8&quot;&gt;App Store&lt;/a&gt; &lt;br /&gt;
Android: &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.tea.game&amp;amp;hl=en&quot;&gt;Google Play Store&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 13 Oct 2016 07:07:00 -0500</pubDate>
        <link>https://apollack11.github.io/shape-stream.html</link>
        <guid isPermaLink="true">https://apollack11.github.io/shape-stream.html</guid>
        
        <category>software</category>
        
        <category>shape</category>
        
        <category>stream</category>
        
        
        <category>java</category>
        
        <category>git</category>
        
      </item>
    
  </channel>
</rss>
