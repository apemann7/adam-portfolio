<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Adam Pollack</title>
    <description></description>
    <link>https://apollack11.github.io/</link>
    <atom:link href="https://apollack11.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 29 Mar 2017 11:44:33 -0500</pubDate>
    <lastBuildDate>Wed, 29 Mar 2017 11:44:33 -0500</lastBuildDate>
    <generator>Jekyll v3.4.0</generator>
    
      <item>
        <title>Stabilizing Noisy IMU Data Using Optical Flow</title>
        <description>&lt;p&gt;Using optical flow and an extended Kalman filter to generate more accurate odometry of a Jackal robot.&lt;/p&gt;

&lt;p&gt;Final GitHub Repo: &lt;a href=&quot;https://github.com/apollack11/advanced-computer-vision&quot;&gt;advanced-computer-vision&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In collaboration with Nate Kaiser.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;We demonstrated a system which uses vision processing techniques to improve the estimation of the state of a Jackal UGV from Clearpath Robotics. We applied feature tracking, optical flow, and perspective transform to derive the estimated motion of the robot from a camera alone. This motion estimation is then combined with data from an inertial measurement unit (IMU) in an extended Kalman filter (EKF) through a process called sensor fusion in order to provide a more accurate estimation of the robot’s state. We primarily focused on solutions which are computationally cheap and can be implemented on the Jackal’s onboard computer.&lt;/p&gt;

&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;The Jackal UGV is made primarily to be an entry-level research robot which has an onboard computer, GPS, and inertial measurement unit (IMU) integrated with the Robot Operating System (ROS). The inertial measurement unit, which was the focus of our project, contains sensors which measure the orientation, angular velocity, and linear acceleration of the robot. Through our work with Jackal, we found that the IMU on board often takes noisy measurements which contain inaccuracies and drift. We wanted to develop a fully working system which could compensate for this IMU noise by utilizing an optical camera. Through only measurements from the optical camera and the IMU we sought to filter the data gathered about the odometry of the robot in order to make better predictions about its location in the real world.&lt;/p&gt;

&lt;h3 id=&quot;methods&quot;&gt;Methods&lt;/h3&gt;
&lt;h4 id=&quot;code-overview&quot;&gt;Code Overview&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;C++/Python:&lt;/strong&gt; For the computer vision portion, we wrote code in C++ because the algorithms we are using tend to be very computationally intensive. Python was used to perform sensor fusion, tie in functionality from ROS, and collect data when necessary. Using a combination of programming languages allowed us to code for efficiency or speed depending on the application, and with ROS, the integration of these languages was seamless.&lt;br /&gt;
&lt;strong&gt;Robot Operating System (ROS):&lt;/strong&gt; ROS allowed us to write individual nodes which perform different computations, and then pass data easily between nodes using publishers and subscribers. We chose to use ROS because it is commonly applied to robotic systems and because we wrote our code in both C++ and Python, it enabled easier data transfer.&lt;br /&gt;
&lt;strong&gt;OpenCV:&lt;/strong&gt; OpenCV is a commonly used computer vision library which happens to work very well with Python, C++, and ROS, so it was a great choice to handle some of our vision applications.&lt;/p&gt;

&lt;h4 id=&quot;feature-tracking&quot;&gt;Feature Tracking&lt;/h4&gt;
&lt;p&gt;In order to derive motion from a video feed, we first had to determine which features we wanted to track in the image. To do this we looked into the Shi-Tomasi paper titled Good Features to Track&lt;sup&gt;[1]&lt;/sup&gt;. In this paper, Jianbo Shi and Carlo Tomasi outline methods to determine which features of an image are unique enough to be identified in multiple pictures of the same scene. Figure 1 shows the results of the Shi-Tomasi Method.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/feature-tracking.png&quot; alt=&quot;Feature Tracking Results&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;p&gt;Figure 1: Feature Tracking Results&lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;calculating-optical-flow&quot;&gt;Calculating Optical Flow&lt;/h4&gt;
&lt;p&gt;To calculate optical flow, we used the Lucas-Kanade Method. The Lucas-Kanade Method uses the assumption that all neighboring pixels will have similar motion to extract optical flow. Their method assigns a weight function to the pixels and then uses the Weighted Least Squares method to formulate an equation to derive motion&lt;sup&gt;[2]&lt;/sup&gt;.&lt;/p&gt;

&lt;h4 id=&quot;extracting-motion&quot;&gt;Extracting Motion&lt;/h4&gt;
&lt;p&gt;We used perspective transform to extract motion from our optical flow data. The perspective transform method relies on relating the pixel coordinates (u, v) to world coordinates (x, y, z). This relationship is desirable because we already have the motion of the camera in (u, v) coordinates from optical flow so a simple transformation from pixel to world coordinates would provide the information we need. Finding the desired transformation required only taking some measurements relating the camera to the ground.&lt;/p&gt;

&lt;h4 id=&quot;sensor-fusion&quot;&gt;Sensor Fusion&lt;/h4&gt;
&lt;p&gt;Sensor fusion is the combination of measurements from multiple different sensors to create a more accurate measurement&lt;sup&gt;[3]&lt;/sup&gt;. The more accurate estimation is derived using an Extended Kalman Filter based on the input measurements. Since the goal of our project is to stabilize noisy IMU data, we looked at performing sensor fusion using data from the inertial measurement unit on board Jackal and the extracted motion from our optical flow calculations above. The figure below shows the basic principle behind sensor fusion in which two sets of data (pink and red) are fused to form the estimation (green).&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/sensor-fusion-example.png&quot; alt=&quot;Sensor Fusion Example&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;p&gt;Figure 2: Sensor Fusion Example&lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;
&lt;h4 id=&quot;testing-details&quot;&gt;Testing Details&lt;/h4&gt;
&lt;p&gt;We gathered data for four different test cases including stationary, normal driving, aggressive driving, and random driving. The stationary test case consisted of not moving the robot. Normal driving involved driving the robot around at a medium speed and making gradual turns. Aggressive driving included forward and backward motion at the highest speed as well as some quick turns. And finally, random driving consisted of a mix between slow and fast driving and quick and slow turns. For each test, we collected odometry data from the IMU alone, the IMU fused with optical flow data, and the wheel odometry built-in to Jackal’s codebase. Below are three graphs of results we collected.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/stationary_position_estimation.png&quot; alt=&quot;Stationary Position Estimation Results&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;p&gt;Figure 3: Stationary Position Estimation&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Figure 3 shows that the visual-inertial odometry filters out almost all of the noise and drift accrued using the IMU alone. The robot is not moving, so the x-position for all three cases should show a reading very close to zero. The encoder-inertial performed as expected given that the encoders should be reading a constant, zero value for x-position. Our visual-inertial reading performed nearly identically to the encoder-inertial results which suggests that our optical flow calculations improved the IMU data.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/aggressive_position_estimation.png&quot; alt=&quot;Aggressive Driving Position Estimation Results&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;p&gt;Figure 4: Aggressive Driving Position Estimation&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Figure 4 shows the position estimation for the aggressive driving test case. For this case, the IMU data has again drifted completely away from the visual-inertial and encoder-inertial odometry. However, aggressive driving also demonstrated a discrepancy between the encoder and visual odometry cases. The difference between the two results is most likely due to the inaccuracies of the encoder when the wheels are slipping and the inaccuracies of optical flow when at high speeds. The actual position is likely in between these two results.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/random_heading_estimation.png&quot; alt=&quot;Rnadom Driving Heading Estimation Results&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;p&gt;Figure 5: Random Driving Heading Estimation&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Figure 5 is a graph of the heading estimation (yaw-position) for the random movement case. Even though the driving for this case was sporadic and unpredictable, all three odometry estimates track together. This suggests that the IMU alone performs similarly to the visual-inertial and encoder-inertial estimates when measuring rotation.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Our original goal was to filter noisy IMU data using optical flow, and we believe we accomplished this effectively. Compared to inertial odometry alone, visual-inertial odometry was able to limit drift and provide a more accurate estimate of position. Our solution was simple, computationally efficient, and fairly robust as a fully working system. The code was developed using ROS and OpenCV, so it is easily extendable by anyone interested in making modifications or improvements to our results. The code can be found on GitHub &lt;a href=&quot;https://github.com/apollack11/advanced-computer-vision&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Including encoder data into our sensor fusion calculations would improve accuracy by providing another estimation of odometry&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;br /&gt;
[1] Shi, Jianbo, and Tomasi. “Good Features To Track.” Proceedings of IEEE Conference on Computer Vision and Pattern Recognition CVPR-94 (1994): n. pag. Web.&lt;br /&gt;
[2] Wu, Ying. “Differential Motion Analysis (I)”. EECS432 Lecture 5. Tech Room L170, Evanston, IL. 24 Jan. 2017. Lecture.&lt;br /&gt;
[3] Moran, Antonio, Ph.D. &lt;em&gt;Sensor Data Fusion Using Kalman Filters.&lt;/em&gt; IEEE, 24 May 2013. Web. &amp;lt;&lt;a href=&quot;https://www.slideshare.net/antoniomorancardenas/data-fusion-with-kalman-filtering&quot;&gt;URL&lt;/a&gt;&amp;gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 29 Mar 2017 05:29:00 -0500</pubDate>
        <link>https://apollack11.github.io/advanced-computer-vision.html</link>
        <guid isPermaLink="true">https://apollack11.github.io/advanced-computer-vision.html</guid>
        
        <category>Computer</category>
        
        <category>Vision</category>
        
        
        <category>Python</category>
        
        <category>C++</category>
        
        <category>OpenCV</category>
        
      </item>
    
      <item>
        <title>Exploring Methods for Object Recognition</title>
        <description>&lt;p&gt;Implementation of the Bag of Words method for object recognition.&lt;/p&gt;

&lt;p&gt;Final GitHub Repo: &lt;a href=&quot;https://github.com/apollack11/object-recognition&quot;&gt;object-recognition&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-goal&quot;&gt;The Goal&lt;/h3&gt;
&lt;p&gt;I hadn’t gotten a chance to explore machine learning yet at Northwestern, so I decided to center my winter project around the topic. Exploring object recognition seemed like a straightforward yet interesting way to do so. I set out to learn the basics of machine learning and to create a fully-functioning object classifier.&lt;/p&gt;

&lt;h3 id=&quot;methodology&quot;&gt;Methodology&lt;/h3&gt;

&lt;h4 id=&quot;bag-of-words&quot;&gt;Bag of Words&lt;/h4&gt;
&lt;p&gt;The gif above this post shows the results of a trained classifier using the bag of words method. Bag of words is a four step process which consists of the following&lt;sup&gt;[1]&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STEP 1:&lt;/strong&gt; Extract Features&lt;br /&gt;
This step aims to simplify images into a set of smaller features so the classifier doesn’t need to train on every pixel of every image. These features should be unique and should not depend on orientation or scaling. For this step I used the scale-invariant feature transform (SIFT) algorithm implemented through OpenCV. SIFT features are invariant to orientation and uniform scaling and are partially invariant to illumination changes and affine distortion&lt;sup&gt;[2]&lt;/sup&gt;. Due to their robustness, SIFT features seemed ideal for a project which will be demonstrated in a variety of locations with varying lighting conditions and cameras.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STEP 2:&lt;/strong&gt; Learn Visual Vocabulary&lt;br /&gt;
The next step is to learn visual vocabulary from the features extracted in Step 1. To do this, the features are put into clusters using unsupervised learning. Specifically, this step with use K-Means Clustering to determine features which are related to each other. I used K-Means Clustering from the Scikit Learn Python package to do this.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STEP 3:&lt;/strong&gt; Quantize Features using Visual Vocabulary&lt;br /&gt;
Created a histogram of all the features&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STEP 4:&lt;/strong&gt; Train Classifier&lt;br /&gt;
Implemented a Support Vector Machine (SVM) through Scikit Learn which was trained on the extracted features.&lt;/p&gt;

&lt;h4 id=&quot;results&quot;&gt;Results&lt;/h4&gt;
&lt;p&gt;The results of training on Dataset #2 are shown in the gif above. The details surrounding each dataset are below.&lt;/p&gt;

&lt;p&gt;Dataset #1
    - Contains images taken from phone of soda can and screwdriver
    - Training set
        - 51 images of soda can, 35 of screwdriver
    - Test set
        - 5 images of each
    - Accuracy: 0.859375 —&amp;gt; 85.94%&lt;/p&gt;

&lt;p&gt;Dataset #2
    - Contains images taken from webcam of soda can and screwdriver
    - Training set
        - 193 images of each (85.78% of data)
    - Test set
        - 32 images of each (14.22% of data)
    - Accuracy: 0.953125 —&amp;gt; 95.31%&lt;/p&gt;

&lt;h4 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h4&gt;
&lt;p&gt;Relevant GitHub Repo:
&lt;a href=&quot;https://github.com/apollack11/learning-tensorflow&quot;&gt;learning-tensorflow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I also started looking into object recognition through Tensorflow. I started by completing the tutorials on Tensorflow found &lt;a href=&quot;https://www.tensorflow.org/tutorials/deep_cnn&quot;&gt;here&lt;/a&gt;. One of the tutorials focused on recognizing handwritten digits using data from the MNIST dataset. This problem used a convolutional neural network to train a classifier to recognize handwritten digits. I extended the original solution to save the variables that defined the convolutional neural network so the model could be trained and then used to predict based on individual images. I was able to draw handwritten digits in GIMP and predict the number in the image based on the network. &lt;!-- Below is an example image with prediction. --&gt;&lt;/p&gt;

&lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Write custom SVM to train object recognition&lt;/li&gt;
  &lt;li&gt;Improve robustness by collecting a wider variety of images&lt;/li&gt;
  &lt;li&gt;Implement object recognition with the Baxter robot in our lab&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;br /&gt;
[1] Kitani, Kris. “Bag-of-Visual-Words.” &lt;em&gt;Carnegie Mellon University&lt;/em&gt;. Web. &lt;a href=&quot;http://www.cs.cmu.edu/~16385/lectures/Lecture12.pdf&quot;&gt;http://www.cs.cmu.edu/~16385/lectures/Lecture12.pdf&lt;/a&gt;.
[2] Lowe, David G. “Object Recognition from Local Scale-Invariant Features.” &lt;em&gt;Proceedings of the Seventh IEEE International Conference on Computer Vision&lt;/em&gt; (1999): n. pag. Web.&lt;/p&gt;
</description>
        <pubDate>Fri, 17 Mar 2017 22:30:00 -0500</pubDate>
        <link>https://apollack11.github.io/object-recognition.html</link>
        <guid isPermaLink="true">https://apollack11.github.io/object-recognition.html</guid>
        
        <category>Machine</category>
        
        <category>Learning</category>
        
        
        <category>Python</category>
        
        <category>ScikitLearn</category>
        
        <category>Tensorflow</category>
        
      </item>
    
      <item>
        <title>Baxter Pick and Place</title>
        <description>&lt;p&gt;Worked with a team to program a Baxter robot to play the Shell Game using the Robot Operating System.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/BlakeStrebel/shell_game&quot;&gt;Github Repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;More info coming soon!&lt;/p&gt;
</description>
        <pubDate>Wed, 21 Dec 2016 07:26:34 -0600</pubDate>
        <link>https://apollack11.github.io/baxter-pick-and-place.html</link>
        <guid isPermaLink="true">https://apollack11.github.io/baxter-pick-and-place.html</guid>
        
        <category>robotics</category>
        
        
        <category>ROS</category>
        
        <category>python</category>
        
        <category>baxter</category>
        
        <category>git</category>
        
      </item>
    
      <item>
        <title>Hockey Simulation</title>
        <description>&lt;p&gt;Mathematica simulation of a hockey puck hitting the crossbar of a goal.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/hockey_simulation.gif&quot; alt=&quot;Hockey Simulation&quot; style=&quot;width: auto; padding: 10px&quot; /&gt;
&lt;/div&gt;

&lt;h3 id=&quot;description&quot;&gt;Description&lt;/h3&gt;
&lt;p&gt;The goal of this project was to simulate a hockey stick striking a puck at an angle and having the puck hit the crossbar of a goal. The simulation models the system from the side. From this view, there is a hockey stick, a puck, and a goal. The hockey stick is modeled as a pendulum which has an external force acting on it to simulate a player swinging a real hockey stick. The puck is a rectangle from the side view and thus has rotational and translational kinetic energy upon being hit and hitting the crossbar. In reality, the flex of a hockey stick upon impact with the puck would cause the puck to gain the lift necessary for it to travel from ice-level to the height of the crossbar. To simplify things in this simulation, the puck has an initial angle necessary to cover that vertical distance. The crossbar itself is a cylindrical object which viewed from the side is a circle.&lt;/p&gt;

&lt;h3 id=&quot;euler-lagrange-equations&quot;&gt;Euler-Lagrange Equations&lt;/h3&gt;
&lt;p&gt;This dynamic system was modeled using the Euler-Lagrange method which was calculated using the kinetic energy and potential energy of both the stick and the puck. For both I used the twist matrix (V) and the inertia matrix (Itot) to calculate kinetic energy and I used the mass of each and their vertical positions (y) to calculate potential energy.&lt;/p&gt;

&lt;h3 id=&quot;constraints&quot;&gt;Constraints&lt;/h3&gt;
&lt;p&gt;There are two impacts in the system: one in which the hockey stick strikes the puck and another in which the hockey puck strikes the crossbar. For the first impact, I attempted to model my constraint as point at which the front edge of the hockey stick meets the rear edge of the puck. For the second impact, I used the point at which the front edge of the puck meets the x-location of the front of the crossbar as my constraint.&lt;/p&gt;

&lt;h3 id=&quot;external-forces&quot;&gt;External Forces&lt;/h3&gt;
&lt;p&gt;The hockey stick has a constant force applied of 243.5N. This force was
chosen to create a tangential velocity of 100mph (44.704m/s) at the end of the stick when it
contacts the puck.&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Dec 2016 04:46:00 -0600</pubDate>
        <link>https://apollack11.github.io/hockey-simulation.html</link>
        <guid isPermaLink="true">https://apollack11.github.io/hockey-simulation.html</guid>
        
        <category>dynamics</category>
        
        
        <category>mathematica</category>
        
        <category>Euler-Lagrange</category>
        
      </item>
    
      <item>
        <title>Shape Stream</title>
        <description>&lt;p&gt;Shape Stream is a cross-platform mobile game for Android and iOS created in Java. The game was released in April 2016 and has over 400 total downloads.&lt;/p&gt;

&lt;p&gt;I began working on Shape Stream in January 2016 with Tyler Carrara and Eric Robinson, two friends from high school. We wanted to learn how to bring an application from idea to completion and thought that creating a mobile game would be an effective way to do so. None of us majored in Computer Science, but we were all familiar with programming and willing to learn the object-oriented programming necessary to produce the game. The project began with an idea of having the user interact with shapes falling from the top of the screen. Building on this idea, we brainstormed exactly what we wanted to include in the game. We determined how the user would score points, how they would lose lives, when the game would end, what power-ups we should include, and more.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;p style=&quot;font-weight: bold&quot;&gt;Shape Stream Initial Concept Images&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/concept-image1.png&quot; alt=&quot;Shape Stream Concept 1&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/concept-image2.png&quot; alt=&quot;Shape Stream Concept 2&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/concept-image3.png&quot; alt=&quot;Shape Stream Concept 3&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;After sketching out a broad concept and listing features, we began laying out a plan for programming the application. We conducted research to figure out what resources we wanted to use to create the game. Because I already had an intermediate knowledge of Java, we chose to use LibGDX as our game engine and RoboVM to port the game from Android to iOS. These technologies allowed us to focus development all in Java and still create a product which could be released in both app marketplaces. Tyler and I were the ones focused on programming, with Tyler learning Java from scratch and me taking my intermediate knowledge of Java and learning how to apply it to the LibGDX game engine and RoboVM.&lt;/p&gt;

&lt;p&gt;Once we had a basic idea of how the programming was going to work, we created a project and a git repository to store our code and collaborate. Using GitHub, we were able to simultaneously make changes to the project and then merge them together. The basic structure of our project consists of a core directory which contains all code related directly to the game. The core directory includes all menus, text overlays, settings, and the game itself. The project also has desktop, android, and ios directories which allow the game to be instantiated on different devices. The desktop version was used for testing new features related directly to the game, and we used Android and iOS devices to test ads, in-app purchases, and the functionality of the game on touch platforms.&lt;/p&gt;

&lt;p&gt;After three months of work including working through spring break, we launched Shape Stream for Android and iOS on April 16th, 2016. On April 15th, 2016, we participated in the MobiLEHIGH game creation competition at Lehigh University and won first prize by choice of the judges.&lt;/p&gt;

&lt;p&gt;Shape Stream involved combining a wide set of skills in order to move from concept to product. In addition to the code, we also created every asset from scratch including music, sound effects, logos, and sprites. I learned an immense amount about game design, project planning, Java programming, bug fixing, and testing while working on this project.&lt;/p&gt;

&lt;p&gt;We received great feedback from a number of friends and are currently working on a new version to improve upon the original concept. If you want to checkout Shape Stream, links to both app stores are at the top of the post.&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;p style=&quot;font-weight: bold&quot;&gt;Shape Stream Final Images&lt;/p&gt;
&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img src=&quot;/assets/ShapeStreamScreenshot1.png&quot; alt=&quot;Shape Stream Screenshot 1&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/ShapeStreamScreenshot3.png&quot; alt=&quot;Shape Stream Screenshot 2&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/ShapeStreamScreenshot4.png&quot; alt=&quot;Shape Stream Screenshot 3&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
  &lt;img src=&quot;/assets/ShapeStreamScreenshot5.png&quot; alt=&quot;Shape Stream Screenshot 4&quot; style=&quot;width: 300px; padding: 10px&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;iOS: &lt;a href=&quot;https://itunes.apple.com/td/app/shape-stream/id1095099500?mt=8&quot;&gt;App Store&lt;/a&gt; &lt;br /&gt;
Android: &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.tea.game&amp;amp;hl=en&quot;&gt;Google Play Store&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 13 Oct 2016 07:07:00 -0500</pubDate>
        <link>https://apollack11.github.io/shape-stream.html</link>
        <guid isPermaLink="true">https://apollack11.github.io/shape-stream.html</guid>
        
        <category>software</category>
        
        <category>shape</category>
        
        <category>stream</category>
        
        
        <category>java</category>
        
        <category>git</category>
        
      </item>
    
  </channel>
</rss>
